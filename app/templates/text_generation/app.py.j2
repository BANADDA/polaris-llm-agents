from fastapi import FastAPI, Depends, HTTPException, BackgroundTasks
from typing import Dict
import os
import logging
import torch
import asyncio
from concurrent.futures import ThreadPoolExecutor
from huggingface_hub import login

{% if use_llama_cpp %}
from llama_cpp import Llama
{% else %}
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
{% endif %}
from app.auth.middleware import verify_token
from app.auth.token import TokenManager

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()
model_id = os.getenv("MODEL_ID")
generation_pipeline = None

# Thread pool for async processing
executor = ThreadPoolExecutor()

# Initialize model with error handling
try:
    # Try to authenticate with HuggingFace if token is provided
    hf_token = os.getenv("HF_TOKEN")
    if hf_token:
        login(token=hf_token)
        logger.info("Authenticated with HuggingFace")

    {% if use_llama_cpp %}
    logger.info(f"Loading Llama model from: /app/{model_id}")
    model = Llama(
        model_path=f"/app/{model_id}",
        n_gpu_layers=-1,  # Use all GPU layers
        n_ctx=2048        # Context window
    )
    logger.info("Llama model loaded successfully")
    {% else %}
    logger.info(f"Loading model for: {model_id}")
    
    if "OpenELM" in model_id:
        logger.info("Detected OpenELM model, using pipeline approach")
        generation_pipeline = pipeline(
            "text-generation",
            model=model_id,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype=torch.float16
        )
        logger.info("OpenELM pipeline loaded successfully")
    else:
        # Load model in half-precision for faster inference
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            trust_remote_code=True,
            use_auth_token=hf_token if hf_token else None,
            device_map="auto",  # Use CUDA if available
            torch_dtype=torch.float16  # Half-precision
        )
        tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            trust_remote_code=True,
            use_auth_token=hf_token if hf_token else None
        )
        # Set pad_token to avoid warnings
        tokenizer.pad_token = tokenizer.eos_token
        logger.info("Model and tokenizer loaded successfully")
    {% endif %}
except Exception as e:
    logger.error(f"Error loading model: {str(e)}")
    raise RuntimeError(f"Failed to load model: {str(e)}")

# Helper function for synchronous generation
def generate_sync(prompt: str):
    try:
        {% if use_llama_cpp %}
        logger.info("Generating response with Llama model")
        output = model(
            prompt,
            max_tokens=100,
            temperature=0.7,
            top_p=0.95,
            echo=False
        )
        return {"response": output['choices'][0]['text']}
        {% else %}
        if generation_pipeline is not None:
            logger.info("Generating response with OpenELM pipeline")
            output = generation_pipeline(
                prompt,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.8,
                top_k=50,
                top_p=0.95
            )
            # Extract only the new generated text, removing the prompt
            generated_text = output[0]['generated_text']
            response_text = generated_text[len(prompt):] if generated_text.startswith(prompt) else generated_text
            return {"response": response_text}
        else:
            logger.info("Generating response with Transformers model")
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,  # Reduced for faster generation
                do_sample=True,
                temperature=0.8,
                top_k=50,
                top_p=0.95,
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True  # Stop early if possible
            )
            response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Remove the prompt from the response
            response_text = response_text[len(prompt):] if response_text.startswith(prompt) else response_text
            return {"response": response_text}
        {% endif %}
    except Exception as e:
        logger.error(f"Error during inference: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# Asynchronous wrapper for generation
async def run_generation(prompt: str):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, generate_sync, prompt)

@app.post("/inference")
async def generate(prompt: str, background_tasks: BackgroundTasks, token: str = Depends(verify_token)):
    task = await run_generation(prompt)
    return task

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model_id": model_id}

@app.get("/cuda_status")
async def cuda_status():
    return {"cuda_available": torch.cuda.is_available()}