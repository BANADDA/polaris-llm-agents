2025-02-11 16:04:30,697 - INFO - Initializing SSH client.
2025-02-11 16:04:30,697 - INFO - Attempting SSH connection to 24.83.13.62:15000
2025-02-11 16:04:33,449 - INFO - SSH connection established successfully.
2025-02-11 16:04:33,465 - INFO - Initializing SFTP connection
2025-02-11 16:04:35,059 - INFO - SFTP connection established successfully
2025-02-11 16:04:35,059 - INFO - Executing command: docker ps
2025-02-11 16:04:35,859 - DEBUG - Command output: CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
2025-02-11 16:04:35,859 - INFO - Checking for npm installation
2025-02-11 16:04:35,859 - INFO - Executing command: which npm
2025-02-11 16:04:37,024 - DEBUG - Command output: /usr/bin/npm
2025-02-11 16:04:37,024 - INFO - Checking for localtunnel installation
2025-02-11 16:04:37,025 - INFO - Executing command: which lt
2025-02-11 16:04:38,188 - DEBUG - Command output: /usr/local/bin/lt
2025-02-11 16:04:38,188 - INFO - Selected random port: 8562
2025-02-11 16:04:38,188 - INFO - Executing command: mkdir -p /tmp/deploy_a7bb9a4b
2025-02-11 16:04:39,382 - INFO - Executing command: mkdir -p /tmp/deploy_a7bb9a4b/app/auth
2025-02-11 16:04:40,602 - INFO - Executing command: mkdir -p /tmp/deploy_a7bb9a4b/app/api
2025-02-11 16:04:41,763 - INFO - Writing content to /tmp/deploy_a7bb9a4b/app/__init__.py
2025-02-11 16:04:43,234 - INFO - File write completed successfully
2025-02-11 16:04:43,234 - INFO - Writing content to /tmp/deploy_a7bb9a4b/app/auth/__init__.py
2025-02-11 16:04:43,980 - INFO - File write completed successfully
2025-02-11 16:04:43,981 - INFO - Writing content to /tmp/deploy_a7bb9a4b/app/api/__init__.py
2025-02-11 16:04:44,699 - INFO - File write completed successfully
2025-02-11 16:04:44,699 - INFO - Writing content to /tmp/deploy_a7bb9a4b/Dockerfile
2025-02-11 16:04:45,820 - INFO - File write completed successfully
2025-02-11 16:04:45,821 - INFO - Writing content to /tmp/deploy_a7bb9a4b/requirements.txt
2025-02-11 16:04:46,920 - INFO - File write completed successfully
2025-02-11 16:04:46,938 - INFO - Writing content to /tmp/deploy_a7bb9a4b/app/app.py
2025-02-11 16:04:48,039 - INFO - File write completed successfully
2025-02-11 16:04:48,040 - INFO - Writing content to /tmp/deploy_a7bb9a4b/app/auth/middleware.py
2025-02-11 16:04:49,159 - INFO - File write completed successfully
2025-02-11 16:04:49,161 - INFO - Writing content to /tmp/deploy_a7bb9a4b/app/auth/token.py
2025-02-11 16:04:50,259 - INFO - File write completed successfully
2025-02-11 16:04:50,261 - INFO - Writing content to /tmp/deploy_a7bb9a4b/app/auth/config.json
2025-02-11 16:04:51,361 - INFO - File write completed successfully
2025-02-11 16:04:51,361 - INFO - Writing content to /tmp/deploy_a7bb9a4b/app/api/inference.py
2025-02-11 16:04:52,493 - INFO - File write completed successfully
2025-02-11 16:04:52,493 - INFO - Writing content to /tmp/deploy_a7bb9a4b/app/api/token.py
2025-02-11 16:04:53,607 - INFO - File write completed successfully
2025-02-11 16:04:53,608 - INFO - Executing command: cd /tmp/deploy_a7bb9a4b && docker build -t dzanm1onxouyv5nau9bslbfd9sz2-gpt2:50b91d33 .
2025-02-11 16:04:54,958 - ERROR - Command error output: DEPRECATED: The legacy builder is deprecated and will be removed in a future release.
            Install the buildx component to build images with BuildKit:
            https://docs.docker.com/go/buildx/
2025-02-11 16:04:54,958 - DEBUG - Command output: Sending build context to Docker daemon  36.86kB
Step 1/13 : FROM nvidia/cuda:11.8.0-runtime-ubuntu22.04
 ---> d8fb74ecc8b2
Step 2/13 : WORKDIR /app
 ---> Using cache
 ---> 1c1526cd4839
Step 3/13 : ENV MODEL_ID="gpt2"
 ---> Using cache
 ---> 990d09f4d8d7
Step 4/13 : ENV USER_ID="dzanm1onxouyv5nau9bslbfd9sz2"
 ---> Using cache
 ---> 5ddd2e81e174
Step 5/13 : ENV PYTHONPATH=/app
 ---> Using cache
 ---> 47f967e10f33
Step 6/13 : ENV HF_TOKEN="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"
 ---> Using cache
 ---> 271a23198e9f
Step 7/13 : RUN apt-get update && apt-get install -y     python3-pip     git     cmake     ninja-build     build-essential     pkg-config     gcc     g++
 ---> Using cache
 ---> 11016618d2bb
Step 8/13 : COPY requirements.txt .
 ---> Using cache
 ---> 94133892a6ef
Step 9/13 : RUN pip3 install -r requirements.txt
 ---> Using cache
 ---> d6567ca2af8f
Step 10/13 : RUN pip install -U bitsandbytes
 ---> Using cache
 ---> 8fe1d84eb56e
Step 11/13 : COPY . .
 ---> 391acd7e9f83
Step 12/13 : RUN echo '#!/bin/bash\ncd /app && exec uvicorn app.app:app --host 0.0.0.0 --port 8000 --timeout-keep-alive 120' > /entrypoint.sh &&     chmod +x /entrypoint.sh
 ---> Running in 64d6f4146072
 ---> Removed intermediate container 64d6f4146072
 ---> ea46e9d0e0dd
Step 13/13 : ENTRYPOINT ["/entrypoint.sh"]
 ---> Running in 324f3b4a0064
 ---> Removed intermediate container 324f3b4a0064
 ---> 84315ed02c80
Successfully built 84315ed02c80
Successfully tagged dzanm1onxouyv5nau9bslbfd9sz2-gpt2:50b91d33
2025-02-11 16:04:54,958 - INFO - Executing command: docker images dzanm1onxouyv5nau9bslbfd9sz2-gpt2:50b91d33 --quiet
2025-02-11 16:04:56,343 - DEBUG - Command output: 84315ed02c80
2025-02-11 16:04:56,343 - INFO - Executing command: docker run -d -p 8562:8000 -e HF_TOKEN=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 dzanm1onxouyv5nau9bslbfd9sz2-gpt2:50b91d33
2025-02-11 16:04:57,969 - DEBUG - Command output: 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb
2025-02-11 16:04:57,969 - INFO - Executing command: docker inspect --format='{{.State.Status}}' 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb
2025-02-11 16:04:59,201 - DEBUG - Command output: running
2025-02-11 16:04:59,201 - INFO - Executing command: docker inspect --format='{{json .NetworkSettings}}' 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb
2025-02-11 16:05:00,388 - DEBUG - Command output: {"Bridge":"","SandboxID":"c626661b43165398f9f3cac6e1ce785840aeaf328f4dbbe7f884abb6db68f0a3","SandboxKey":"/run/snap.docker/netns/c626661b4316","Ports":{"8000/tcp":[{"HostIp":"0.0.0.0","HostPort":"8562"},{"HostIp":"::","HostPort":"8562"}]},"HairpinMode":false,"LinkLocalIPv6Address":"","LinkLocalIPv6PrefixLen":0,"SecondaryIPAddresses":null,"SecondaryIPv6Addresses":null,"EndpointID":"883081a97798bbe08ac73577545ce4df203fb8c2c4c15597f7c9e163f95bb812","Gateway":"172.17.0.1","GlobalIPv6Address":"","GlobalIPv6PrefixLen":0,"IPAddress":"172.17.0.2","IPPrefixLen":16,"IPv6Gateway":"","MacAddress":"02:42:ac:11:00:02","Networks":{"bridge":{"IPAMConfig":null,"Links":null,"Aliases":null,"MacAddress":"02:42:ac:11:00:02","NetworkID":"1c865ec36cb0ef317dadb73394f7c28da5f104dd7e3ec5c114a1cf2f32c31880","EndpointID":"883081a97798bbe08ac73577545ce4df203fb8c2c4c15597f7c9e163f95bb812","Gateway":"172.17.0.1","IPAddress":"172.17.0.2","IPPrefixLen":16,"IPv6Gateway":"","GlobalIPv6Address":"","GlobalIPv6PrefixLen":0,"DriverOpts":null,"DNSNames":null}}}
2025-02-11 16:05:00,388 - INFO - Creating tunnel for port 8562 with subdomain gpt2-79137
2025-02-11 16:05:00,388 - INFO - Executing command: mkdir -p /tmp/tunnels
2025-02-11 16:05:01,623 - INFO - Executing command: touch /tmp/tunnels/tunnel_gpt2-79137_8562.log
2025-02-11 16:05:02,856 - INFO - Executing command: chmod 777 /tmp/tunnels/tunnel_gpt2-79137_8562.log
2025-02-11 16:05:04,044 - INFO - Executing command: nohup lt --port 8562 --subdomain gpt2-79137 > /tmp/tunnels/tunnel_gpt2-79137_8562.log 2>&1 & echo $!
2025-02-11 16:05:05,222 - DEBUG - Command output: 271771
2025-02-11 16:05:05,222 - INFO - Started tunnel process with PID: 271771
2025-02-11 16:05:08,228 - INFO - Executing command: cat /tmp/tunnels/tunnel_gpt2-79137_8562.log
2025-02-11 16:05:09,010 - DEBUG - Command output: your url is: https://gpt2-79137.loca.lt
2025-02-11 16:05:09,010 - INFO - Tunnel created successfully: https://gpt2-79137.loca.lt
2025-02-11 16:05:09,010 - INFO - Writing content to /tmp/tunnels/tunnel_gpt2-79137_8562.pid
2025-02-11 16:05:10,557 - INFO - File write completed successfully
2025-02-11 16:05:12,071 - INFO - Executing command: rm -rf /tmp/deploy_a7bb9a4b
2025-02-11 16:05:12,861 - INFO - Closing SFTP connection
2025-02-11 16:05:12,861 - INFO - Closing SSH connection
2025-02-11 16:05:12,861 - INFO - All connections closed successfully
2025-02-11 16:05:12,861 - INFO - Initializing SSH client.
2025-02-11 16:05:12,861 - INFO - Attempting SSH connection to 24.83.13.62:15000
2025-02-11 16:05:15,707 - INFO - SSH connection established successfully.
2025-02-11 16:05:15,708 - INFO - Initializing SFTP connection
2025-02-11 16:05:17,223 - INFO - SFTP connection established successfully
2025-02-11 16:05:17,223 - INFO - Executing command: docker logs 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb 2>&1
2025-02-11 16:05:18,048 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 16:05:23,061 - INFO - Executing command: docker logs 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb 2>&1
2025-02-11 16:05:23,881 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 16:05:28,888 - INFO - Executing command: docker logs 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb 2>&1
2025-02-11 16:05:29,699 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 16:05:34,713 - INFO - Executing command: docker logs 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb 2>&1
2025-02-11 16:05:35,547 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 16:05:40,562 - INFO - Executing command: docker logs 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb 2>&1
2025-02-11 16:05:41,386 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 16:05:46,391 - INFO - Executing command: docker logs 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb 2>&1
2025-02-11 16:05:47,278 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 16:05:52,287 - INFO - Executing command: docker logs 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb 2>&1
2025-02-11 16:05:53,179 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 16:05:58,196 - INFO - Executing command: docker logs 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb 2>&1
2025-02-11 16:05:59,007 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 16:06:04,008 - INFO - Executing command: docker logs 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb 2>&1
2025-02-11 16:06:04,812 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 16:06:09,842 - INFO - Executing command: docker logs 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb 2>&1
2025-02-11 16:06:10,631 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 16:06:15,638 - INFO - Executing command: docker logs 39852412101f4724b7e47ad7b36b519367a5471ffdd54bed36a76d688299bddb 2>&1
2025-02-11 16:06:16,445 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     172.17.0.1:51396 - "OPTIONS /token?user_id=dzanm1onxouyv5nau9bslbfd9sz2&model_id=gpt2 HTTP/1.1" 200 OK
INFO:     172.17.0.1:51396 - "GET /token?user_id=dzanm1onxouyv5nau9bslbfd9sz2&model_id=gpt2 HTTP/1.1" 405 Method Not Allowed
2025-02-11 16:06:21,457 - INFO - Closing SFTP connection
2025-02-11 16:06:21,457 - INFO - Closing SSH connection
2025-02-11 16:06:21,457 - INFO - All connections closed successfully
2025-02-11 16:59:08,320 - INFO - Initializing SSH client.
2025-02-11 16:59:08,320 - INFO - Attempting SSH connection to 24.83.13.62:15000
2025-02-11 16:59:11,245 - INFO - SSH connection established successfully.
2025-02-11 16:59:11,245 - INFO - Initializing SFTP connection
2025-02-11 16:59:12,820 - INFO - SFTP connection established successfully
2025-02-11 16:59:12,820 - INFO - Executing command: docker ps
2025-02-11 16:59:13,635 - DEBUG - Command output: CONTAINER ID   IMAGE                                        COMMAND            CREATED          STATUS          PORTS                                       NAMES
39852412101f   dzanm1onxouyv5nau9bslbfd9sz2-gpt2:50b91d33   "/entrypoint.sh"   54 minutes ago   Up 54 minutes   0.0.0.0:8562->8000/tcp, :::8562->8000/tcp   naughty_dewdney
2025-02-11 16:59:13,635 - INFO - Checking for npm installation
2025-02-11 16:59:13,635 - INFO - Executing command: which npm
2025-02-11 16:59:14,828 - DEBUG - Command output: /usr/bin/npm
2025-02-11 16:59:14,828 - INFO - Checking for localtunnel installation
2025-02-11 16:59:14,828 - INFO - Executing command: which lt
2025-02-11 16:59:16,037 - DEBUG - Command output: /usr/local/bin/lt
2025-02-11 16:59:16,037 - INFO - Selected random port: 8878
2025-02-11 16:59:16,037 - INFO - Executing command: mkdir -p /tmp/deploy_31eec9f4
2025-02-11 16:59:17,257 - INFO - Executing command: mkdir -p /tmp/deploy_31eec9f4/app/auth
2025-02-11 16:59:18,443 - INFO - Executing command: mkdir -p /tmp/deploy_31eec9f4/app/api
2025-02-11 16:59:19,673 - INFO - Writing content to /tmp/deploy_31eec9f4/app/__init__.py
2025-02-11 16:59:21,206 - INFO - File write completed successfully
2025-02-11 16:59:21,206 - INFO - Writing content to /tmp/deploy_31eec9f4/app/auth/__init__.py
2025-02-11 16:59:21,950 - INFO - File write completed successfully
2025-02-11 16:59:21,950 - INFO - Writing content to /tmp/deploy_31eec9f4/app/api/__init__.py
2025-02-11 16:59:22,703 - INFO - File write completed successfully
2025-02-11 16:59:22,703 - INFO - Writing content to /tmp/deploy_31eec9f4/Dockerfile
2025-02-11 16:59:23,822 - INFO - File write completed successfully
2025-02-11 16:59:23,822 - INFO - Writing content to /tmp/deploy_31eec9f4/requirements.txt
2025-02-11 16:59:24,933 - INFO - File write completed successfully
2025-02-11 16:59:24,949 - INFO - Writing content to /tmp/deploy_31eec9f4/app/app.py
2025-02-11 16:59:26,085 - INFO - File write completed successfully
2025-02-11 16:59:26,086 - INFO - Writing content to /tmp/deploy_31eec9f4/app/auth/middleware.py
2025-02-11 16:59:27,219 - INFO - File write completed successfully
2025-02-11 16:59:27,234 - INFO - Writing content to /tmp/deploy_31eec9f4/app/auth/token.py
2025-02-11 16:59:28,343 - INFO - File write completed successfully
2025-02-11 16:59:28,343 - INFO - Writing content to /tmp/deploy_31eec9f4/app/auth/config.json
2025-02-11 16:59:29,517 - INFO - File write completed successfully
2025-02-11 16:59:29,519 - INFO - Writing content to /tmp/deploy_31eec9f4/app/api/inference.py
2025-02-11 16:59:30,645 - INFO - File write completed successfully
2025-02-11 16:59:30,647 - INFO - Writing content to /tmp/deploy_31eec9f4/app/api/token.py
2025-02-11 16:59:31,762 - INFO - File write completed successfully
2025-02-11 16:59:31,762 - INFO - Executing command: cd /tmp/deploy_31eec9f4 && docker build -t dzanm1onxouyv5nau9bslbfd9sz2-gpt2-large:d579d67b .
2025-02-11 16:59:33,145 - ERROR - Command error output: DEPRECATED: The legacy builder is deprecated and will be removed in a future release.
            Install the buildx component to build images with BuildKit:
            https://docs.docker.com/go/buildx/
2025-02-11 16:59:33,145 - DEBUG - Command output: Sending build context to Docker daemon  36.86kB
Step 1/13 : FROM nvidia/cuda:11.8.0-runtime-ubuntu22.04
 ---> d8fb74ecc8b2
Step 2/13 : WORKDIR /app
 ---> Using cache
 ---> 1c1526cd4839
Step 3/13 : ENV MODEL_ID="gpt2-large"
 ---> Using cache
 ---> df8a477c1659
Step 4/13 : ENV USER_ID="dzanm1onxouyv5nau9bslbfd9sz2"
 ---> Using cache
 ---> 3b9cd1250ef0
Step 5/13 : ENV PYTHONPATH=/app
 ---> Using cache
 ---> f3d6686528fc
Step 6/13 : ENV HF_TOKEN="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"
 ---> Using cache
 ---> 6eedcc0bb62d
Step 7/13 : RUN apt-get update && apt-get install -y     python3-pip     git     cmake     ninja-build     build-essential     pkg-config     gcc     g++
 ---> Using cache
 ---> cbd8e133d7ae
Step 8/13 : COPY requirements.txt .
 ---> Using cache
 ---> 85a1a39c748c
Step 9/13 : RUN pip3 install -r requirements.txt
 ---> Using cache
 ---> 0024bfee1ee7
Step 10/13 : RUN pip install -U bitsandbytes
 ---> Using cache
 ---> d6d4693e93f9
Step 11/13 : COPY . .
 ---> 60139d376de9
Step 12/13 : RUN echo '#!/bin/bash\ncd /app && exec uvicorn app.app:app --host 0.0.0.0 --port 8000 --timeout-keep-alive 120' > /entrypoint.sh &&     chmod +x /entrypoint.sh
 ---> Running in b41481716df4
 ---> Removed intermediate container b41481716df4
 ---> 3fac737ffe64
Step 13/13 : ENTRYPOINT ["/entrypoint.sh"]
 ---> Running in 6cb1ea9c493e
 ---> Removed intermediate container 6cb1ea9c493e
 ---> 4aa3fd55e392
Successfully built 4aa3fd55e392
Successfully tagged dzanm1onxouyv5nau9bslbfd9sz2-gpt2-large:d579d67b
2025-02-11 16:59:33,145 - INFO - Executing command: docker images dzanm1onxouyv5nau9bslbfd9sz2-gpt2-large:d579d67b --quiet
2025-02-11 16:59:34,570 - DEBUG - Command output: 4aa3fd55e392
2025-02-11 16:59:34,570 - INFO - Executing command: docker run -d -p 8878:8000 -e HF_TOKEN=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 dzanm1onxouyv5nau9bslbfd9sz2-gpt2-large:d579d67b
2025-02-11 16:59:36,123 - DEBUG - Command output: fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa
2025-02-11 16:59:36,123 - INFO - Executing command: docker inspect --format='{{.State.Status}}' fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa
2025-02-11 16:59:37,368 - DEBUG - Command output: running
2025-02-11 16:59:37,368 - INFO - Executing command: docker inspect --format='{{json .NetworkSettings}}' fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa
2025-02-11 16:59:38,608 - DEBUG - Command output: {"Bridge":"","SandboxID":"0e50483092040e08fd9c7560febdd1eeda837f965638141660710a5d5f16fc2b","SandboxKey":"/run/snap.docker/netns/0e5048309204","Ports":{"8000/tcp":[{"HostIp":"0.0.0.0","HostPort":"8878"},{"HostIp":"::","HostPort":"8878"}]},"HairpinMode":false,"LinkLocalIPv6Address":"","LinkLocalIPv6PrefixLen":0,"SecondaryIPAddresses":null,"SecondaryIPv6Addresses":null,"EndpointID":"33d494549554e02a2be1be3ee3a7a58b65590ad4bb10bf2c6b44ccdd0cdab074","Gateway":"172.17.0.1","GlobalIPv6Address":"","GlobalIPv6PrefixLen":0,"IPAddress":"172.17.0.3","IPPrefixLen":16,"IPv6Gateway":"","MacAddress":"02:42:ac:11:00:03","Networks":{"bridge":{"IPAMConfig":null,"Links":null,"Aliases":null,"MacAddress":"02:42:ac:11:00:03","NetworkID":"1c865ec36cb0ef317dadb73394f7c28da5f104dd7e3ec5c114a1cf2f32c31880","EndpointID":"33d494549554e02a2be1be3ee3a7a58b65590ad4bb10bf2c6b44ccdd0cdab074","Gateway":"172.17.0.1","IPAddress":"172.17.0.3","IPPrefixLen":16,"IPv6Gateway":"","GlobalIPv6Address":"","GlobalIPv6PrefixLen":0,"DriverOpts":null,"DNSNames":null}}}
2025-02-11 16:59:38,608 - INFO - Creating tunnel for port 8878 with subdomain gpt2-large-17802
2025-02-11 16:59:38,608 - INFO - Executing command: mkdir -p /tmp/tunnels
2025-02-11 16:59:39,820 - INFO - Executing command: touch /tmp/tunnels/tunnel_gpt2-large-17802_8878.log
2025-02-11 16:59:41,029 - INFO - Executing command: chmod 777 /tmp/tunnels/tunnel_gpt2-large-17802_8878.log
2025-02-11 16:59:42,254 - INFO - Executing command: nohup lt --port 8878 --subdomain gpt2-large-17802 > /tmp/tunnels/tunnel_gpt2-large-17802_8878.log 2>&1 & echo $!
2025-02-11 16:59:43,469 - DEBUG - Command output: 273660
2025-02-11 16:59:43,485 - INFO - Started tunnel process with PID: 273660
2025-02-11 16:59:46,468 - INFO - Executing command: cat /tmp/tunnels/tunnel_gpt2-large-17802_8878.log
2025-02-11 16:59:47,275 - WARNING - URL not found in logs on attempt 1
2025-02-11 16:59:50,272 - INFO - Executing command: cat /tmp/tunnels/tunnel_gpt2-large-17802_8878.log
2025-02-11 16:59:51,073 - WARNING - URL not found in logs on attempt 2
2025-02-11 16:59:54,074 - INFO - Executing command: cat /tmp/tunnels/tunnel_gpt2-large-17802_8878.log
2025-02-11 16:59:54,846 - DEBUG - Command output: your url is: https://gpt2-large-17802.loca.lt
2025-02-11 16:59:54,846 - INFO - Tunnel created successfully: https://gpt2-large-17802.loca.lt
2025-02-11 16:59:54,846 - INFO - Writing content to /tmp/tunnels/tunnel_gpt2-large-17802_8878.pid
2025-02-11 16:59:56,412 - INFO - File write completed successfully
2025-02-11 16:59:57,906 - INFO - Executing command: rm -rf /tmp/deploy_31eec9f4
2025-02-11 16:59:58,727 - INFO - Closing SFTP connection
2025-02-11 16:59:58,727 - INFO - Closing SSH connection
2025-02-11 16:59:58,727 - INFO - All connections closed successfully
2025-02-11 16:59:58,727 - INFO - Initializing SSH client.
2025-02-11 16:59:58,727 - INFO - Attempting SSH connection to 24.83.13.62:15000
2025-02-11 17:00:01,702 - INFO - SSH connection established successfully.
2025-02-11 17:00:01,702 - INFO - Initializing SFTP connection
2025-02-11 17:00:03,240 - INFO - SFTP connection established successfully
2025-02-11 17:00:03,240 - INFO - Executing command: docker logs fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa 2>&1
2025-02-11 17:00:04,084 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2-large
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
2025-02-11 17:00:09,093 - INFO - Executing command: docker logs fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa 2>&1
2025-02-11 17:00:09,921 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2-large
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
2025-02-11 17:00:14,922 - INFO - Executing command: docker logs fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa 2>&1
2025-02-11 17:00:15,715 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2-large
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
2025-02-11 17:00:20,711 - INFO - Executing command: docker logs fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa 2>&1
2025-02-11 17:00:21,545 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2-large
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 17:00:26,562 - INFO - Executing command: docker logs fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa 2>&1
2025-02-11 17:00:27,363 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2-large
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 17:00:32,380 - INFO - Executing command: docker logs fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa 2>&1
2025-02-11 17:00:33,196 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2-large
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 17:00:38,198 - INFO - Executing command: docker logs fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa 2>&1
2025-02-11 17:00:39,015 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2-large
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 17:00:44,014 - INFO - Executing command: docker logs fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa 2>&1
2025-02-11 17:00:44,833 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2-large
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 17:00:49,842 - INFO - Executing command: docker logs fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa 2>&1
2025-02-11 17:00:50,645 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2-large
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 17:00:55,649 - INFO - Executing command: docker logs fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa 2>&1
2025-02-11 17:00:56,478 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2-large
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 17:01:01,474 - INFO - Executing command: docker logs fd50c91b6439dc041dc7257749c34f768ee3281576b157053c399bad629d65fa 2>&1
2025-02-11 17:01:02,287 - DEBUG - Command output: INFO:app.app:Loading model and tokenizer for: gpt2-large
INFO:app.app:Loading model using transformers backend
INFO:app.app:Loading standard tokenizer
INFO:app.app:Loading model in CPU mode with optimizations
INFO:app.app:Model loaded successfully
INFO:app.app:Model loaded successfully using transformers backend
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-02-11 17:01:07,301 - INFO - Closing SFTP connection
2025-02-11 17:01:07,301 - INFO - Closing SSH connection
2025-02-11 17:01:07,301 - INFO - All connections closed successfully
